{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to ElasticSearch cluster `docker-cluster`\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from json import dumps\n",
    "\n",
    "es = Elasticsearch(\"http://127.0.0.1:9200\")\n",
    "\n",
    "print(f\"Connected to ElasticSearch cluster `{es.info().body['cluster_name']}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inserción de 1 millón de documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se revisará la carga y el tamaño de almacenamiento de 1 millón de documentos indexados en la base de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La carga de documentos se llevará a cabo mediante el script de python ```load-documents-elastic.py``` localizado en el mismo directorio de este cuaderno. Para cargar la cantidad de datos en cuestión se ejecuta el siguiente comando:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python load-documents-elastic.py -n 1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** es recomendable ejecutar el comando en una terminal aparte de este cuaderno, ya que se trata de una operación que dependiendo la cantidad de documentos, puede llegar a ocupar una considerable cantidad de memoria del sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cargando 1 millón de documentos](../imagenes-de-soporte/elastic-insert/load_1M_documents_elastic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en la imagen superior, cargar 1 millón de documentos ha tomado un tiempo de **4 minutos y 26.16 segundos**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez cargados los documentos en la base de datos, podemos verificar la cantidad total de documentos en el índice ```extracto_cuenta_1m``` de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actualmente existen 1000000 documentos en el índice 'extracto_cuenta_1m'\n"
     ]
    }
   ],
   "source": [
    "cnt = es.cat.count(index=\"extracto_cuenta_1m\", format= \"json\")[0]['count']\n",
    "print(f\"Actualmente existen {cnt} documentos en el índice 'extracto_cuenta_1m'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tamaño de almacenamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener las estadísticas de uso de disco por parte del índice ```extracto_cuenta_1m``` en la base de datos ejecutaremos el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"store_size\": \"138.3mb\",\n",
      " \"store_size_in_bytes\": 145089787,\n",
      " \"all_fields\": {\n",
      "  \"total\": \"137.5mb\",\n",
      "  \"total_in_bytes\": 144187007,\n",
      "  \"inverted_index\": {\n",
      "   \"total\": \"19mb\",\n",
      "   \"total_in_bytes\": 19932341\n",
      "  },\n",
      "  \"stored_fields\": \"87.7mb\",\n",
      "  \"stored_fields_in_bytes\": 91994865,\n",
      "  \"doc_values\": \"12.8mb\",\n",
      "  \"doc_values_in_bytes\": 13503952,\n",
      "  \"points\": \"15.9mb\",\n",
      "  \"points_in_bytes\": 16755849,\n",
      "  \"norms\": \"1.9mb\",\n",
      "  \"norms_in_bytes\": 2000000,\n",
      "  \"term_vectors\": \"0b\",\n",
      "  \"term_vectors_in_bytes\": 0,\n",
      "  \"knn\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "r = dumps(es.indices.disk_usage(index=\"extracto_cuenta_1m\", run_expensive_tasks=True)['extracto_cuenta_1m'], indent=1)\n",
    "print(r[:500])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrayendo información importante de los datos recién obtenidos. El tamaño de almacenamiento de 1 documento de 1 millón de registros con un peso en bruto de 188.5 mb, **una vez comprimido en la base de datos es 138.3mb**. Esto nos da un radio de compresión de 1.36 y una reducción de tamaño en memoria del 26.59%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cargando 1 millón de documentos](../imagenes-de-soporte/elastic-insert/storage_kibana_1M_documents.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inserción de 10 millones de documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se revisará la carga y el tamaño de almacenamiento de 10 millones de documentos indexados en la base de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La carga de documentos se llevará a cabo mediante el script de python ```load-documents-elastic.py``` localizado en el mismo directorio de este cuaderno. Para cargar la cantidad de datos en cuestión se ejecuta el siguiente comando:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python load-documents-elastic.py -n 10\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** es recomendable ejecutar el comando en una terminal aparte de este cuaderno, ya que se trata de una operación que dependiendo la cantidad de documentos, puede llegar a ocupar una considerable cantidad de memoria del sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cargando 1 millón de documentos](../imagenes-de-soporte/elastic-insert/load_10M_documents_elastic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en la imagen superior, cargar 10 millones de documentos ha tomado un tiempo de **48 minutos y 52.61 segundos**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez cargados los documentos en la base de datos, podemos verificar la cantidad total de documentos en el índice ```extracto_cuenta_10m``` de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actualmente existen 10000000 documentos en el índice 'extracto_cuenta_10m'\n"
     ]
    }
   ],
   "source": [
    "cnt = es.cat.count(index=\"extracto_cuenta_10m\", format= \"json\")[0]['count']\n",
    "print(f\"Actualmente existen {cnt} documentos en el índice 'extracto_cuenta_10m'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tamaño de almacenamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener las estadísticas de uso de disco por parte del índice ```extracto_cuenta_10m``` en la base de datos ejecutaremos el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"store_size\": \"1.3gb\",\n",
      " \"store_size_in_bytes\": 1440202391,\n",
      " \"all_fields\": {\n",
      "  \"total\": \"1.3gb\",\n",
      "  \"total_in_bytes\": 1438808499,\n",
      "  \"inverted_index\": {\n",
      "   \"total\": \"191.3mb\",\n",
      "   \"total_in_bytes\": 200694994\n",
      "  },\n",
      "  \"stored_fields\": \"876.7mb\",\n",
      "  \"stored_fields_in_bytes\": 919322081,\n",
      "  \"doc_values\": \"128.7mb\",\n",
      "  \"doc_values_in_bytes\": 135007131,\n",
      "  \"points\": \"156.1mb\",\n",
      "  \"points_in_bytes\": 163784293,\n",
      "  \"norms\": \"19mb\",\n",
      "  \"norms_in_bytes\": 20000000,\n",
      "  \"term_vectors\": \"0b\",\n",
      "  \"term_vectors_in_bytes\": 0\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "r = dumps(es.indices.disk_usage(index=\"extracto_cuenta_10m\", run_expensive_tasks=True)['extracto_cuenta_10m'], indent=1)\n",
    "print(r[:500])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrayendo información importante de los datos recién obtenidos. El tamaño de almacenamiento de 10 documento de 1 millón de registros con un peso en bruto de 1.9 gb, **una vez comprimido en la base de datos es 1.34 gb**. Esto nos da un radio de compresión de 1.42 y una reducción de tamaño en memoria del 29.47%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cargando 1 millón de documentos](../imagenes-de-soporte/elastic-insert/kibana_10M_documents.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7e5a1f785ed82844e2da5d30522181462e1597dbd1807cbc4c5c0cc1d5a2e0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
